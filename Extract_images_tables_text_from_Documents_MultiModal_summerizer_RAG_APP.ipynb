{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b92c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install \"unstructured[all-docs]\" pillow pydantic lxml matplotlib unstructured-pytesseract tesseract-ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b807c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain_core langchain_openai langchain chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1601a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo apt-get update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8774a127",
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo apt-get install poppler-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae3d5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo apt-get install libleptonica-dev tesseract-ocr libtesseract-dev python-pil tesseract-ocr-eng tesseract-ocr-script-latn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317b8ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured_partition.pdf import partition_pdf\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from google.colab import userdata\n",
    "import base64\n",
    "from langchain_core.messages import HumanMessage\n",
    "import uuid\n",
    "\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import io\n",
    "import re\n",
    "from IPython.display import HTML, display\n",
    "from PIL import Image\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed95afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_TOKEN = userdata.get(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7914e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=\"/content/data/cj.pdf\",  # Manatory\n",
    "    strategy=\"hi_res\",   # mandatory to use \"hi_res\" strategy\n",
    "    extract_images_in_pdf = True,   # mandatory to set as \"True\"\n",
    "    extract_image_block_types = [\"Image\", \"Table\"],   # Optional\n",
    "    extract_image_block_to_payload = False,   # Optional\n",
    "    extract_image_block_output_dir = \"extracted_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3804d81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Header = []\n",
    "Footer = []\n",
    "Title = []\n",
    "NarrativeText = []\n",
    "Text = []\n",
    "ListItem = []\n",
    "\n",
    "for element in raw_pdf_elements:\n",
    "    if \"unstructured.documents.elements.Hearder\" in str(type(element)):\n",
    "        Header.append(str(element))\n",
    "\n",
    "    elif \"unstructured.documents.elements.Footer\" in str(type(element)):\n",
    "        Footer.append(str(element))\n",
    "\n",
    "    elif \"unstructured.documents.elements.Title\" in str(type(element)):\n",
    "        Title.append(str(element))\n",
    "\n",
    "    elif \"unstructured.documents.elements.NarrativeText\" in str(type(element)):\n",
    "        NarrativeText.append(str(element))\n",
    "\n",
    "    elif \"unstructured.documents.elements.Text\" in str(type(element)):\n",
    "        Text.append(str(element))\n",
    "\n",
    "    elif \"unstructured.documents.elements.ListItem\" in str(type(element)):\n",
    "        ListItem.append(str(element))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aedec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = []\n",
    "\n",
    "for element in raw_pdf_elements:\n",
    "    if \"unstructured.documents.elements.Image\" in str(type(element)):\n",
    "        img.append(str(element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e863471",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_pdf_elements_2 = partition_pdf(\n",
    "    filename = \".content/data2/Retrieval-Augmented-Generation-for-NLP.pdf\",\n",
    "    strategy = \"hi_res\",\n",
    "    extract_images_in_pdf = True,\n",
    "    extreat_image_block_types = [\"Image\", \"Table\"],\n",
    "    extract_image_block_to_payload = False,\n",
    "    extract_image_block_output_dir = \"extracted_data_2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2486e94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_2 = []\n",
    "\n",
    "for element in raw_pdf_elements_2:\n",
    "    if \"unstructured.documents.elements.Image\" in str(type(element)):\n",
    "        img_2.append(str(element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc99b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_2 = []\n",
    "\n",
    "for element in raw_pdf_elements:\n",
    "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "        table_2.append(str(element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defbc10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NarrativeText_2 = []\n",
    "\n",
    "for element in raw_pdf_elements:\n",
    "    if \"unstructured.documents.elements.NarrativeText\" in str(type(element)):\n",
    "        NarrativeText_2.append(str(element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9928ddae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ListItem_2 = []\n",
    "\n",
    "for element in raw_pdf_elements:\n",
    "    if \"unstructured.documents.elements.ListItem\" in str(type(element)):\n",
    "        ListItem_2.append(str(element))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0241176e",
   "metadata": {},
   "source": [
    "Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7089a658",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = \"\"\" You are an assistant tasked with summarizing texts or tables for retrieval. \\\n",
    "    These summaries will be embedded and uses to retrieve the raw text or table elements. \\\n",
    "    Give a concise summary of the text or table that is well optimized for retrieval\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbff784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(prompt_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc6bec7",
   "metadata": {},
   "source": [
    "Text Summary chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a203bd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(temperature = 0, model=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef9351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer_chain = {\"element\": lambda x : x} | prompt | model | StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a821d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_summaries = []\n",
    "text_summaries = summarizer_chain.batch(Text, {\"max_concurrency\":5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5355abbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_summaries = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2d22fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_summaries = summarizer_chain.batch(table_2, {\"max_concurrency\":5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce677117",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5b5587",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd4ffaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_image(image_path):\n",
    "    \"\"\" Getting The Base64 String \"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7445c386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_summarize(img_base64, prompt):\n",
    "    \"\"\" Make Image Summary \"\"\"\n",
    "    chat = ChatOpenAI(model=\"gpt-4-vision-preview\", max_tokens=1024)\n",
    "\n",
    "    msg = chat.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\":prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64, {img_base64}\"}\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return msg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473d2862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_img_summaries(path):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images\n",
    "    path: Path to list of .jpg files extracted by unstructured\n",
    "    \"\"\"\n",
    "\n",
    "    # Store base64 encoded images\n",
    "    img_base64_list = []\n",
    "\n",
    "    # Store image summaries\n",
    "    image_summaries = []\n",
    "\n",
    "    # Prompt\n",
    "    prompt = \"\"\" \n",
    "\n",
    "    You are an assistant tasked with summarizing images for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw image. \\\n",
    "    Give a concise summary of the image that is well optimized for retrieval.\n",
    "    \"\"\"\n",
    "\n",
    "    base64_image = encoder_image(path)\n",
    "    img_base64_list.append(base64_image)\n",
    "    image_summaries.append(image_summarize(base64_image, prompt))\n",
    "\n",
    "    return img_base64_list, image_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce0dd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/content/extracted_data_2/figure-17-4.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7896a603",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_base64_list, image_summaries = generate_img_summaries(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100b7b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_summaries[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d1f700",
   "metadata": {},
   "source": [
    "Creating MultiVector Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efda917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_vector_retriever(vector_store, text_summaries, texts, table_summaries, tables, image_summaries, images):\n",
    "    \"\"\" Creating retriever that indexes summaries, but retruns raw images or texts\"\"\"\n",
    "\n",
    "    # Initialize the storage layer\n",
    "    store = InMemoryStore()\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    # Creating the multi-vector retriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore= = vector_store,\n",
    "        docstore = store,\n",
    "        id_key = id_key\n",
    "    ) \n",
    "\n",
    "    # Helper function to add documents to the vector store and doc_store\n",
    "\n",
    "    def add_documents(retriever, doc_summaries, doc_contents):\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "\n",
    "        summary_docs = [\n",
    "            Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "            for i, s in enumerate(doc_summaries)\n",
    "        ]\n",
    "\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "\n",
    "        # Add texts, tables and images\n",
    "        # Check that text_summaries is not empty before adding\n",
    "        if text_summaries:\n",
    "            add_documents(retriever, text_summaries, texts)\n",
    "\n",
    "        # Check that table_summaries is not empty before adding\n",
    "        if table_summaries:\n",
    "            add_documents(retriever, table_summaries, table_2)\n",
    "\n",
    "        # Check that image_summaries is not empty before adding\n",
    "        if image_summaries:\n",
    "            add_documents(retriever, image_summaries, img)\n",
    "\n",
    "    return retriever\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045ae886",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(\n",
    "    collection_name=\"mm_rag\", embedding_function=OpenAIEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbf83be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating retriever \n",
    "retriever_multi_vector_img = create_multi_vector_retriever(\n",
    "    vector_store,\n",
    "    text_summaries,\n",
    "    Text,\n",
    "    table_summaries,\n",
    "    table_2,\n",
    "    image_summaries,\n",
    "    img_base64_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47f6ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_img_based64(img_based64):\n",
    "    \"\"\" Display base64 encoded string as image \"\"\"\n",
    "    # Create an HTML img tag with the base64 string as the source\n",
    "    image_html = f\"cing src = 'data:image/jpeg;base64, {img_based64}' />\"\n",
    "\n",
    "    # Display the image by rendering the HTML\n",
    "    display(HTML(image_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc7f9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_img_based64(img_base64_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c83761",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_summaries[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44b766a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def looks_like_base64(sb):\n",
    "    \"\"\" Check if the string looks like base64 \"\"\"\n",
    "    return re.match(\"^[A-Za-z0-9+/] + [*]{0, 2}$\", sb) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1e3c8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_image_data(b64data):\n",
    "    \"\"\" Check if the base64 data is an image by looking at the start of the data \"\"\"\n",
    "\n",
    "    image_signatures = {\n",
    "        b\"\\xFF\\xDB\\xFF\": \"jpg\",\n",
    "        b\"\\x89\\x50\\x4E\\x47\\x0D\\x0A\\x1A\\x0A\": \"png\",\n",
    "        b\"\\x47\\x49\\x46\\x38\" : \"gif\",\n",
    "        b\"\\x52\\x49\\x46\\x46\" : \"webp\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        header = base64.b64decode(b64data)[:8] # Decode and get the first 8 bytes\n",
    "        for sig, format in image_signatures.items():\n",
    "            if header.startswith(sig):\n",
    "                return True\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29d5ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_base64_image(base64_string, size=(128, 128)):\n",
    "    \"\"\" Resize an image encoded as a Base64 string \"\"\"\n",
    "\n",
    "    # Decode the Base64 string\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = img.resize(size=size, Image.LANCZOS)\n",
    "\n",
    "    # Save the resized image to a bytes buffer\n",
    "    buffered = io.BytesIO()\n",
    "    resized_img.save(buffered, format=img.format)\n",
    "\n",
    "    # Encode the resized image to Base64\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c4a8b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_image_text_types(docs):\n",
    "    \"\"\"Split base64-encoded images and texts\"\"\" \n",
    "\n",
    "    b64_images = []\n",
    "    texts = []\n",
    "    for doc in docs:\n",
    "        # Check if the document is of type Document and extract page_content if so\n",
    "        if isinstance(doc, Document):\n",
    "            doc = doc.page_content\n",
    "        if looks_like_base64(dqc) and is_image_data(doc):\n",
    "            doc = resize_base64_image(doc, size=(1300, 600))\n",
    "            b64_images.append(doc)\n",
    "        else:\n",
    "            texts.append(doc)\n",
    "    print(b64_images)\n",
    "    print(texts)\n",
    "\n",
    "    return {\"images\": b64_images, \"texts\": texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91276d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_prompt_func(data_dict):\n",
    "    \"\"\"\n",
    "    Join the context into a single string\n",
    "    \"\"\"\n",
    "    print(data_dict)\n",
    "    formatted_texts = \"\\n\". join(data_dict[\"context\"][\"texts\"])\n",
    "    messages = []\n",
    "\n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        for image in data_dict[\"context\"][\"images\"]:\n",
    "            image_message = {\n",
    "            \"type\": \"image_ur1\",\n",
    "            \"image_url\": {\"url\": f\"data:image/jpeg;base64, {image}\"},\n",
    "\n",
    "            }\n",
    "            messages.append(image_message)\n",
    "\n",
    "    # Adding the text for analysis\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"You are a helpful assistant. \\n\"\n",
    "            \"You will be given a mixed info(s) .\\n\"\n",
    "            \"Use this information to provide relevant information to the user quetion. \\n\"\n",
    "            f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
    "            \"Text and / or tables: \\n\"\n",
    "            f\"{formatted_texts}\"\n",
    "        ),\n",
    "\n",
    "    }\n",
    "    \n",
    "    messages.append(text_message)\n",
    "\n",
    "    return [HumanMessage(content=messages)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba96e23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_modal_rag_chain(retriever):\n",
    "    \"\"\" Multi-Modal RAG Chan \"\"\"\n",
    "\n",
    "    # Multi-Modal LLM\n",
    "\n",
    "    model =ChatOpenAI(temperature=0, model=\"gpt-4-vision-preview\", max_tokens=1024)\n",
    "\n",
    "    # RAG pipeline\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\" : retriever | RunnableLambda(split_image_text_types),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "\n",
    "        }\n",
    "\n",
    "        | RunnableLambda(img_prompt_func)\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debfab6c",
   "metadata": {},
   "source": [
    "Create RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7554e151",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_multimodal_rag = multi_modal_rag_chain(retriever_multi_vector_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba07f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Explain any images / figures in the paper with Left: NQ performance as more documents are retrieved. Center: Retrieval recall performance\" \\\n",
    "\"in NQ, Right: MS-MARCO Bleu-1 and Rough-L as more documents are retrieved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee460798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run RAG chain \n",
    "chain_multimodal_rag.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76248cae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
